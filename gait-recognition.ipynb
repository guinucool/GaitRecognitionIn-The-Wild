{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WALK ON: GAIT RECOGNITION IN-THE-WILD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The widespreading of wearable and portable IoT devices is greatly improving quality of life thanks to the development of 5G and data analysis techniques. Every device we use can collect data regarding our daily activities and use it to help improve their practice. However this can also lead to great security and privacy vulnerabilities when the data handling procedure is not carried out correctly.\n",
    "\n",
    "With the data collected from devices we already carry every day (like our smartphones), it is possible to collect gait data that can be used to identify activities or individuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has the objetive to use gait data collected from different subject's phones to create and train a deep learning model that will be capable of identifying users given their gait information.\n",
    "\n",
    "In particular data from the accelerometer and gyroscope will be used to detect signal sequences relative to a step. These will then be classified to identify different users depending on their walking patterns.\n",
    "\n",
    "This problem can be divided in two smaller tasks i.e. extraction of the signal relative to a step (this can be done with standard signal processing techniques), and then classification using a neural network.\n",
    "\n",
    "The former will be addressed with the procedure proposed in [1], which was developed for mobile phone sensors, but can easily be extended to any other wearable device. Classification, on the other hand, will be addressed with a simple CNN.\n",
    "\n",
    "[1] Gadaleta, Matteo, and Michele Rossi. \"Idnet: Smartphone-based gait recognition with convolutional neural networks.\" Pattern Recognition 74 (2018): 25-37."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets used for the exploration and development of this project are given by [2]. There are four datasets dedicated to the goals this project wants to achieve, so every one of them will be used in order to more accurately measure the results obtained by the model developed.\n",
    "\n",
    "[2] Zou Q, Wang Y, Zhao Y, Wang Q and Li Q, Deep learning-based gait recogntion using smartphones in the wild, IEEE Transactions on Information Forensics and Security, vol. 15, no. 1, pp. 3197-3212, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the acquisition, reading and conversion of any dataset, a set of utility functions were set to better auxiliate in the reading and conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['README.md', 'utils.py', '.gitignore', '.git', 'gait-recognition.ipynb', 'datasets']\n",
      "./datasets/dataset1/train_acc_x.txt\n",
      "datasets/dataset1/train/Inertial\\ \\Signals/train_acc_y.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/dataset1/train/Inertial\\\\ \\\\Signals/train_acc_y.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(signals)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 44\u001b[0m final \u001b[38;5;241m=\u001b[39m \u001b[43mload_gait_information\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./datasets/dataset1/train_acc_x.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/dataset1/train/Inertial\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSignals/train_acc_y.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatasets/dataset1/train/Inertial\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSignals/train_acc_z.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(final))\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(final)\n",
      "Cell \u001b[0;32mIn[20], line 26\u001b[0m, in \u001b[0;36mload_gait_information\u001b[0;34m(x, y, z)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Open the dataset file location\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Read every row of the axis readings\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m     30\u001b[0m \n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Conversion of the row values and addition to the axis matrix\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/dataset1/train/Inertial\\\\ \\\\Signals/train_acc_y.txt'"
     ]
    }
   ],
   "source": [
    "def load_gait_information(x: str, y: str, z: str):\n",
    "    '''\n",
    "    Read all the x, y and z values of a dataset of a given device readings.\n",
    "\n",
    "    Args:\n",
    "        x (str): The path for the dataset of x readings.\n",
    "        y (str): The path for the dataset of y readings.\n",
    "        z (str): The path for the dataset of z readings.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The 3 * Steps * 128 matrix of the read information.\n",
    "    '''\n",
    "\n",
    "    # Create the final matrix where the readings will be stored\n",
    "    signals = []\n",
    "\n",
    "    # Iterative process for every axis reading\n",
    "    for path in [x, y, z]:\n",
    "\n",
    "        # Create a temporary matrix for storing a single axis readings\n",
    "        axis = []\n",
    "\n",
    "        print(path)\n",
    "\n",
    "        # Open the dataset file location\n",
    "        file = open(path, 'r')\n",
    "\n",
    "        # Read every row of the axis readings\n",
    "        for row in file:\n",
    "\n",
    "            # Conversion of the row values and addition to the axis matrix\n",
    "            axis.append(np.array(row.strip().split(' '), dtype=np.float32))\n",
    "\n",
    "        # Addition of the axis matrix to the readings matrix\n",
    "        signals.append(axis)\n",
    "\n",
    "        # Closing of the read file\n",
    "        file.close()\n",
    "\n",
    "    # Returning and conversion of the final readings matrix\n",
    "    return np.array(signals)\n",
    "\n",
    "print(os.listdir('.'))\n",
    "final = load_gait_information('./datasets/dataset1/train_acc_x.txt','datasets/dataset1/train/Inertial\\ \\Signals/train_acc_y.txt','datasets/dataset1/train/Inertial\\ \\Signals/train_acc_z.txt')\n",
    "print(len(final))\n",
    "print(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 - 118 Subjects, Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  datasets/dataset1.zip\n",
      "   creating: datasets/dataset1/Dataset #1/\n",
      "   creating: datasets/dataset1/Dataset #1/test/\n",
      "   creating: datasets/dataset1/Dataset #1/test/Inertial Signals/\n",
      "  inflating: datasets/dataset1/Dataset #1/test/Inertial Signals/test_acc_x.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/test/Inertial Signals/test_acc_y.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/test/Inertial Signals/test_acc_z.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/test/Inertial Signals/test_gyr_x.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/test/Inertial Signals/test_gyr_y.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/test/Inertial Signals/test_gyr_z.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/test/y_test.txt  \n",
      "   creating: datasets/dataset1/Dataset #1/train/\n",
      "   creating: datasets/dataset1/Dataset #1/train/Inertial Signals/\n",
      "  inflating: datasets/dataset1/Dataset #1/train/Inertial Signals/train_acc_x.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/train/Inertial Signals/train_acc_y.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/train/Inertial Signals/train_acc_z.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/train/Inertial Signals/train_gyr_x.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/train/Inertial Signals/train_gyr_y.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/train/Inertial Signals/train_gyr_z.txt  \n",
      "  inflating: datasets/dataset1/Dataset #1/train/y_train.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.usercontent.google.com/download?id=1yPNWWeubU9SjRKqW7H4-q0BaNsw57jWc&export=download&authuser=0&confirm=t&uuid=f7baf6b2-4139-40d1-b1c1-9ff21f5d696d&at=AIrpjvMKaZOBaOKN_juwrHVVepG3%3A1737392898709' -O datasets/dataset1.zip\n",
    "\n",
    "!unzip datasets/dataset1.zip -d datasets/dataset1\n",
    "\n",
    "!find datasets/dataset1 -mindepth 2 -type f -exec mv -t datasets/dataset1 {} +\n",
    "\n",
    "!find datasets/dataset1 -type d -mindepth 1 -exec rm -r {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 - 20 Subjects, Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #3 - 118 Subjects, Time-fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #4 - 20 Subjects, Time-fixed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
